{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM18zrbG73oWaISSLUSfUU9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs==0.5.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRXovjwZ7trn",
        "outputId": "2fa34779-34ec-4aa8-823c-c07ad9bd2cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs==0.5.1\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from nnfs==0.5.1) (1.26.4)\n",
            "Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0WoAKHT7XdE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "class Layer_Dense:\n",
        "    def __init__ (self, n_inputs, n_neurons):\n",
        "        self.weights = np.random.randn(n_inputs, n_neurons) * 0.01\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
        "\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "class Activation_ReLU:\n",
        "    def __init__ (self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.output = np.maximum(0.0, inputs)\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dinputs = dvalues.copy()\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "class Activation_Softmax:\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "\n",
        "        # single_output은 한개의 배치에 대한, 각 인풋에 따른 softMax값\n",
        "        # single_dvalues는 각 인풋에 따른 손실함수의 값\n",
        "\n",
        "        # 각 배치에서 matrix 형태가 나오니, 배치를 합산하면 3D 이 문제를 다시 2D로 합산하기 위해, jacobian_matrix를 각 gradient로 내적을 시켜 합산함.\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
        "\n",
        "class Loss:\n",
        "    def calculate (self, output, y):\n",
        "        sample_losses = self.forward(output, y)\n",
        "        # print('sample_losses : ', sample_losses[:5])\n",
        "        data_loss = np.mean(sample_losses)\n",
        "        return data_loss\n",
        "\n",
        "    def backward (self, dvalues, y_true):\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # 분류해야할 클래스의 갯수, 마지막에 나오는 features 의 갯수.\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        if len(y_true) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "    def forward(self, y_pred, y_true):\n",
        "        samples = len(y_pred)\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
        "\n",
        "        if len(y_true.shape) == 1:\n",
        "            # y_true is Classifified Vector\n",
        "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "\n",
        "        elif len(y_true.shape) == 2:\n",
        "            # y_true is OneHotEncoded Vector\n",
        "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
        "\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    def forward(self, inputs, y_true):\n",
        "        self.activation.forward(inputs)\n",
        "        self.output = self.activation.output\n",
        "\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        samples = len(dvalues)\n",
        "\n",
        "\n",
        "        # onehotencoding 된 y_true를 다시, 인덱스화 시킴.\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        self.dinputs = dvalues.copy()\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "\n",
        "        self.dinputs = self.dinputs / samples;\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer_SGD:\n",
        "    def __init__(self, learning_rate = 1.0, decay = 0., momentum = 0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_larning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        if self.momentum:\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_larning_rate * layer.dweights\n",
        "\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums -\\\n",
        "                self.current_larning_rate * layer.dbiases\n",
        "\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        else :\n",
        "            weight_updates = -self.current_larning_rate  * layer.dweights\n",
        "            bias_updates = -self.current_larning_rate * layer.dbiases\n",
        "\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_larning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "class Optimizer_Adagrad:\n",
        "    def __init__(self, learning_rate = 1.0, decay = 0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_larning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        layer.weight_cache += layer.dweights ** 2\n",
        "        layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "        layer.weights += -self.current_larning_rate * layer.dweights \\\n",
        "                    / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "\n",
        "        layer.biases += -self.current_larning_rate * layer.dbiases \\\n",
        "                    / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_larning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "class Optimizer_RMSprop:\n",
        "\n",
        "\n",
        "    # RMSprop에서 학습률 감쇠가 느리기 때문에, learning rate 1 은 불안정성을 초래함.\n",
        "\n",
        "    def __init__(self, learning_rate = 0.001, decay = 0., epsilon=1e-7, rho = 0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_larning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho\n",
        "\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "\n",
        "        # rho 가 클수록 과거의 값이 더 많이 반영됨. 일반적으로 0.9로 조정됨.\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "        layer.weights += -self.current_larning_rate * layer.dweights \\\n",
        "                    / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "\n",
        "        layer.biases += -self.current_larning_rate * layer.dbiases \\\n",
        "                    / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_larning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "class Optimizer_Adam:\n",
        "    # RMSprop에서 학습률 감쇠가 느리기 때문에, learning rate 1 은 불안정성을 초래함.\n",
        "\n",
        "    def __init__(self, learning_rate = 0.001, decay = 0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_larning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                layer.weight_momentums + \\\n",
        "                                (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                                layer.bias_momentums + \\\n",
        "                                (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "        # 편향을 보정하는 것임. 초반에는 많이 곱하고, 후반에 그대로 사용하는것임.\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "                (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "                (1 - self.beta_1 ** (self.iterations + 1))\n",
        "\n",
        "        # 여기는 RMSprops와 같음.\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "                (1 - self.beta_2 ) * layer.dweights ** 2\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "                (1 - self.beta_2 ) * layer.dbiases ** 2\n",
        "\n",
        "        # 편향을 보정하는 것임. 초반에는 많이 곱하고, 후반에 그대로 사용하는것임.\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "            (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "            (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_larning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "        layer.biases += -self.current_larning_rate * \\\n",
        "                         bias_momentums_corrected / \\\n",
        "                         (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "\n",
        "\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_larning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "metadata": {
        "id": "TCwVIZJk8f0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nnfs\n",
        "nnfs.init()\n",
        "\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "X, y = spiral_data(100, 3)\n",
        "\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "loss_function = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# optimizer = Optimizer_SGD(learning_rate=1., decay=0.001, momentum=0.9)\n",
        "# optimizer = Optimizer_Adagrad(learning_rate=1., decay=0.0001)\n",
        "# optimizer = Optimizer_RMSprop(learning_rate=0.02, decay=1e-5, rho = 0.999)\n",
        "optimizer = Optimizer_Adam(learning_rate=0.02, decay=1e-5, epsilon=1e-7, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "for epoch in range(10001):\n",
        "    dense1.forward(X)\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    dense2.forward(activation1.output)\n",
        "    loss = loss_function.forward(dense2.output, y)\n",
        "\n",
        "    predictions = np.argmax(loss_function.output, axis= 1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "\n",
        "    accuracy = np.mean(predictions == y)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch: {epoch}, ' +\n",
        "                f'Acc: {accuracy:.3f}, ' +\n",
        "              f'Loss: {loss:.3f}, ' +\n",
        "              f'learn_rate: {optimizer.current_larning_rate}')\n",
        "\n",
        "    loss_function.backward(loss_function.output, y)\n",
        "    dense2.backward(loss_function.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12ueLmgB9cbE",
        "outputId": "2b68a49a-5621-45cd-f7e8-adab198f5eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Acc: 0.360, Loss: 1.099, learn_rate: 0.02\n",
            "Epoch: 100, Acc: 0.673, Loss: 0.769, learn_rate: 0.01998021958261321\n",
            "Epoch: 200, Acc: 0.813, Loss: 0.552, learn_rate: 0.019960279044701046\n",
            "Epoch: 300, Acc: 0.840, Loss: 0.445, learn_rate: 0.019940378268975763\n",
            "Epoch: 400, Acc: 0.863, Loss: 0.380, learn_rate: 0.01992051713662487\n",
            "Epoch: 500, Acc: 0.873, Loss: 0.336, learn_rate: 0.01990069552930875\n",
            "Epoch: 600, Acc: 0.890, Loss: 0.308, learn_rate: 0.019880913329158343\n",
            "Epoch: 700, Acc: 0.890, Loss: 0.287, learn_rate: 0.019861170418772778\n",
            "Epoch: 800, Acc: 0.907, Loss: 0.269, learn_rate: 0.019841466681217078\n",
            "Epoch: 900, Acc: 0.910, Loss: 0.250, learn_rate: 0.01982180200001982\n",
            "Epoch: 1000, Acc: 0.913, Loss: 0.238, learn_rate: 0.019802176259170884\n",
            "Epoch: 1100, Acc: 0.910, Loss: 0.230, learn_rate: 0.01978258934311912\n",
            "Epoch: 1200, Acc: 0.913, Loss: 0.219, learn_rate: 0.01976304113677013\n",
            "Epoch: 1300, Acc: 0.920, Loss: 0.210, learn_rate: 0.019743531525483964\n",
            "Epoch: 1400, Acc: 0.910, Loss: 0.202, learn_rate: 0.01972406039507293\n",
            "Epoch: 1500, Acc: 0.917, Loss: 0.192, learn_rate: 0.019704627631799327\n",
            "Epoch: 1600, Acc: 0.920, Loss: 0.187, learn_rate: 0.019685233122373254\n",
            "Epoch: 1700, Acc: 0.937, Loss: 0.184, learn_rate: 0.019665876753950384\n",
            "Epoch: 1800, Acc: 0.930, Loss: 0.172, learn_rate: 0.01964655841412981\n",
            "Epoch: 1900, Acc: 0.933, Loss: 0.167, learn_rate: 0.019627277990951823\n",
            "Epoch: 2000, Acc: 0.933, Loss: 0.162, learn_rate: 0.019608035372895814\n",
            "Epoch: 2100, Acc: 0.933, Loss: 0.158, learn_rate: 0.01958883044887805\n",
            "Epoch: 2200, Acc: 0.933, Loss: 0.154, learn_rate: 0.019569663108249594\n",
            "Epoch: 2300, Acc: 0.943, Loss: 0.151, learn_rate: 0.01955053324079414\n",
            "Epoch: 2400, Acc: 0.953, Loss: 0.150, learn_rate: 0.019531440736725945\n",
            "Epoch: 2500, Acc: 0.940, Loss: 0.145, learn_rate: 0.019512385486687673\n",
            "Epoch: 2600, Acc: 0.947, Loss: 0.142, learn_rate: 0.019493367381748363\n",
            "Epoch: 2700, Acc: 0.950, Loss: 0.140, learn_rate: 0.019474386313401298\n",
            "Epoch: 2800, Acc: 0.947, Loss: 0.137, learn_rate: 0.019455442173562\n",
            "Epoch: 2900, Acc: 0.950, Loss: 0.135, learn_rate: 0.019436534854566128\n",
            "Epoch: 3000, Acc: 0.947, Loss: 0.132, learn_rate: 0.01941766424916747\n",
            "Epoch: 3100, Acc: 0.940, Loss: 0.138, learn_rate: 0.019398830250535893\n",
            "Epoch: 3200, Acc: 0.953, Loss: 0.129, learn_rate: 0.019380032752255354\n",
            "Epoch: 3300, Acc: 0.947, Loss: 0.127, learn_rate: 0.01936127164832186\n",
            "Epoch: 3400, Acc: 0.947, Loss: 0.125, learn_rate: 0.01934254683314152\n",
            "Epoch: 3500, Acc: 0.947, Loss: 0.124, learn_rate: 0.019323858201528515\n",
            "Epoch: 3600, Acc: 0.950, Loss: 0.123, learn_rate: 0.019305205648703173\n",
            "Epoch: 3700, Acc: 0.947, Loss: 0.122, learn_rate: 0.01928658907028997\n",
            "Epoch: 3800, Acc: 0.947, Loss: 0.120, learn_rate: 0.01926800836231563\n",
            "Epoch: 3900, Acc: 0.950, Loss: 0.119, learn_rate: 0.019249463421207133\n",
            "Epoch: 4000, Acc: 0.960, Loss: 0.120, learn_rate: 0.019230954143789846\n",
            "Epoch: 4100, Acc: 0.953, Loss: 0.118, learn_rate: 0.019212480427285565\n",
            "Epoch: 4200, Acc: 0.953, Loss: 0.118, learn_rate: 0.019194042169310647\n",
            "Epoch: 4300, Acc: 0.963, Loss: 0.119, learn_rate: 0.019175639267874092\n",
            "Epoch: 4400, Acc: 0.950, Loss: 0.118, learn_rate: 0.019157271621375684\n",
            "Epoch: 4500, Acc: 0.953, Loss: 0.115, learn_rate: 0.0191389391286041\n",
            "Epoch: 4600, Acc: 0.963, Loss: 0.114, learn_rate: 0.019120641688735073\n",
            "Epoch: 4700, Acc: 0.950, Loss: 0.115, learn_rate: 0.019102379201329525\n",
            "Epoch: 4800, Acc: 0.953, Loss: 0.112, learn_rate: 0.01908415156633174\n",
            "Epoch: 4900, Acc: 0.957, Loss: 0.111, learn_rate: 0.01906595868406753\n",
            "Epoch: 5000, Acc: 0.947, Loss: 0.110, learn_rate: 0.01904780045524243\n",
            "Epoch: 5100, Acc: 0.953, Loss: 0.107, learn_rate: 0.019029676780939874\n",
            "Epoch: 5200, Acc: 0.957, Loss: 0.106, learn_rate: 0.019011587562619416\n",
            "Epoch: 5300, Acc: 0.957, Loss: 0.105, learn_rate: 0.01899353270211493\n",
            "Epoch: 5400, Acc: 0.953, Loss: 0.104, learn_rate: 0.018975512101632844\n",
            "Epoch: 5500, Acc: 0.950, Loss: 0.105, learn_rate: 0.018957525663750367\n",
            "Epoch: 5600, Acc: 0.957, Loss: 0.102, learn_rate: 0.018939573291413745\n",
            "Epoch: 5700, Acc: 0.960, Loss: 0.111, learn_rate: 0.018921654887936498\n",
            "Epoch: 5800, Acc: 0.963, Loss: 0.100, learn_rate: 0.018903770356997706\n",
            "Epoch: 5900, Acc: 0.957, Loss: 0.100, learn_rate: 0.018885919602640248\n",
            "Epoch: 6000, Acc: 0.960, Loss: 0.098, learn_rate: 0.018868102529269144\n",
            "Epoch: 6100, Acc: 0.960, Loss: 0.098, learn_rate: 0.018850319041649778\n",
            "Epoch: 6200, Acc: 0.967, Loss: 0.097, learn_rate: 0.018832569044906263\n",
            "Epoch: 6300, Acc: 0.960, Loss: 0.096, learn_rate: 0.018814852444519702\n",
            "Epoch: 6400, Acc: 0.963, Loss: 0.096, learn_rate: 0.018797169146326564\n",
            "Epoch: 6500, Acc: 0.957, Loss: 0.095, learn_rate: 0.01877951905651696\n",
            "Epoch: 6600, Acc: 0.967, Loss: 0.095, learn_rate: 0.018761902081633034\n",
            "Epoch: 6700, Acc: 0.967, Loss: 0.094, learn_rate: 0.018744318128567278\n",
            "Epoch: 6800, Acc: 0.967, Loss: 0.093, learn_rate: 0.018726767104560903\n",
            "Epoch: 6900, Acc: 0.967, Loss: 0.093, learn_rate: 0.018709248917202218\n",
            "Epoch: 7000, Acc: 0.967, Loss: 0.092, learn_rate: 0.018691763474424996\n",
            "Epoch: 7100, Acc: 0.967, Loss: 0.092, learn_rate: 0.018674310684506857\n",
            "Epoch: 7200, Acc: 0.967, Loss: 0.091, learn_rate: 0.01865689045606769\n",
            "Epoch: 7300, Acc: 0.963, Loss: 0.090, learn_rate: 0.01863950269806802\n",
            "Epoch: 7400, Acc: 0.967, Loss: 0.090, learn_rate: 0.018622147319807447\n",
            "Epoch: 7500, Acc: 0.960, Loss: 0.089, learn_rate: 0.018604824230923075\n",
            "Epoch: 7600, Acc: 0.967, Loss: 0.089, learn_rate: 0.01858753334138793\n",
            "Epoch: 7700, Acc: 0.960, Loss: 0.091, learn_rate: 0.018570274561509396\n",
            "Epoch: 7800, Acc: 0.960, Loss: 0.088, learn_rate: 0.018553047801927663\n",
            "Epoch: 7900, Acc: 0.970, Loss: 0.089, learn_rate: 0.018535852973614212\n",
            "Epoch: 8000, Acc: 0.967, Loss: 0.088, learn_rate: 0.01851868998787026\n",
            "Epoch: 8100, Acc: 0.963, Loss: 0.087, learn_rate: 0.018501558756325222\n",
            "Epoch: 8200, Acc: 0.967, Loss: 0.088, learn_rate: 0.01848445919093522\n",
            "Epoch: 8300, Acc: 0.963, Loss: 0.086, learn_rate: 0.018467391203981567\n",
            "Epoch: 8400, Acc: 0.960, Loss: 0.085, learn_rate: 0.018450354708069265\n",
            "Epoch: 8500, Acc: 0.960, Loss: 0.085, learn_rate: 0.018433349616125496\n",
            "Epoch: 8600, Acc: 0.963, Loss: 0.085, learn_rate: 0.018416375841398172\n",
            "Epoch: 8700, Acc: 0.960, Loss: 0.084, learn_rate: 0.01839943329745444\n",
            "Epoch: 8800, Acc: 0.967, Loss: 0.084, learn_rate: 0.01838252189817921\n",
            "Epoch: 8900, Acc: 0.963, Loss: 0.084, learn_rate: 0.018365641557773718\n",
            "Epoch: 9000, Acc: 0.960, Loss: 0.083, learn_rate: 0.018348792190754044\n",
            "Epoch: 9100, Acc: 0.963, Loss: 0.083, learn_rate: 0.0183319737119497\n",
            "Epoch: 9200, Acc: 0.973, Loss: 0.085, learn_rate: 0.018315186036502167\n",
            "Epoch: 9300, Acc: 0.960, Loss: 0.086, learn_rate: 0.018298429079863496\n",
            "Epoch: 9400, Acc: 0.967, Loss: 0.083, learn_rate: 0.018281702757794862\n",
            "Epoch: 9500, Acc: 0.967, Loss: 0.082, learn_rate: 0.018265006986365174\n",
            "Epoch: 9600, Acc: 0.970, Loss: 0.090, learn_rate: 0.018248341681949654\n",
            "Epoch: 9700, Acc: 0.963, Loss: 0.081, learn_rate: 0.018231706761228456\n",
            "Epoch: 9800, Acc: 0.967, Loss: 0.082, learn_rate: 0.018215102141185255\n",
            "Epoch: 9900, Acc: 0.963, Loss: 0.081, learn_rate: 0.018198527739105907\n",
            "Epoch: 10000, Acc: 0.967, Loss: 0.081, learn_rate: 0.018181983472577025\n"
          ]
        }
      ]
    }
  ]
}